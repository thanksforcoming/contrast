---
title: "Welcome to White Little Lies "
layout: post
---


Machine learning courses generally start with the importance of data and its usage. 
Everybody thinks that data is very important, if you have a lot of data your program runs perfectly. Does it mean that the data is always trustworthy? 
Does the data ever lie? It shouldn't be right, because we collect it from the real-world and real-world is a complex system, 
it can't manipulate us. That is not the case in our world, maybe in matrix.
The truth is that data always lies. Here comes another paradox: [Liar paradox](https://en.wikipedia.org/wiki/Liar_paradox). 
No, I am not going to talk about that. I am going to talk about how data can lie. 

Don't think as manual manipulations on collected data. Of course, in that case, the data doesn't reflect the true values from which it is collected. 
It only reflects the intention of the people who change it. What about the raw, unprocessed data? 
Does it always reflect the main intention that we are working on. It appears that it doesn't. 

You know Google, our beloved search company, has access to lots of collections, images, textes etc. 
Until recently when you have written "White boys" on Google ads, you would obtain ofcourse white boys' related pages. 
What about writing "Black boys'', you would obtain adult topics. The research was conducted by [The Markup](https://themarkup.org/google-the-giant/2020/07/23/google-advertising-keywords-black-girls).After its publication, Google must have blocked the racial searches, but it 
was obvious that Google data has a bias that related the people of color to the sexual assignments while exempting white people.Â  
The surprising thing is that the article was published in 2020, exactly this year. You think that racial behaviours don't exist anymore, but it is proof that they are still here. 
This is an example of racial bias that exists in our data. 

What about frauds, or bots? For example, a [research paper](https://firstmonday.org/ojs/index.php/fm/article/view/7090/5653) 
revealed that 20% of all political tweets were made by bots. These bots can contribute to social communication via social media, can create noise and even can spread misinformation. So how can we be sure if our collected data from Twitter only contains human tweets? 
Without any confirmation about bots' presence, we can't be sure if our data reflects social media conversations. We can't use the data to predict people's opinion on for example US elections. 

What can we do? We first admit that data can be biased and the machine learning system can also be biased and behave unethically because of it. 
There is another aspect of this behavior: the black box systems, but they are next week's topic. 
I have encountered an interesting page [CallingBullshit](https://www.callingbullshit.org/), contains tools, examples to help us critically look the data 
when working on machine learning. I will investigate it!
